{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vis_sounds.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilyzfliu/vis-sounds/blob/main/vis_sounds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUAUAl9GUmTf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from math import pi\n",
        "import torch\n",
        "import imageio\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGVIKB09Urcu",
        "outputId": "ab84641a-dce8-4dc9-9acb-0625b1dd3429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/vis_sound_files/vis-data-simple.zip\n",
        "!cp /content/drive/MyDrive/vis_sound_files/train.txt .\n",
        "!cp /content/drive/MyDrive/vis_sound_files/test.txt ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV3UQ9eMkLCL",
        "outputId": "9f457991-a12d-49e8-a46d-6b5cc256f745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/vis_sound_files/vis-data-simple.zip, /content/drive/MyDrive/vis_sound_files/vis-data-simple.zip.zip or /content/drive/MyDrive/vis_sound_files/vis-data-simple.zip.ZIP.\n",
            "cp: cannot stat '/content/drive/MyDrive/vis_sound_files/train.txt': No such file or directory\n",
            "cp: cannot stat '/content/drive/MyDrive/vis_sound_files/test.txt': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Overview:\n",
        "\n",
        "1. Predicting material from sound (spectrogram) only\n",
        "2. Predicting material(s) from images only\n",
        "3. "
      ],
      "metadata": {
        "id": "AtA0cG7BVLY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "We use the Greatest Hits dataset at https://andrewowens.com/vis/."
      ],
      "metadata": {
        "id": "2lGTQDTOZh02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisSoundsDataset(Dataset):\n",
        "  def __init__(self, split = 'train', mode='sound'):\n",
        "    super(Dataset, self).__init__()\n",
        "    assert split in ['train', 'val', 'test']\n",
        "    self.split = split\n",
        "    assert mode in ['sound', 'image', 'augment']\n",
        "    self.mode = mode\n",
        "\n",
        "    data, labels = self.load_data()\n",
        "    n = len(data)\n",
        "\n",
        "    if self.split == 'train':\n",
        "      data = data[:int(0.8*n)]\n",
        "      labels = data[:int(0.8*n)]\n",
        "    elif self.split == 'val':\n",
        "      data = data[int(0.8*n):]\n",
        "      labels = data[int(0.8*n):]\n",
        "\n",
        "    self.data = {\n",
        "        'inputs': data,\n",
        "        'targets': labels\n",
        "    }\n",
        "\n",
        "    def load_data(self):\n",
        "      if self.split in ['train', 'val']:\n",
        "        splits = 'train.txt'\n",
        "      else:\n",
        "        assert self.split == 'test'\n",
        "        splits = 'text.txt'\n",
        "      with open(splits, 'r') as f:\n",
        "        files = f.readlines()\n",
        "      \n",
        "      data = []\n",
        "      labels = []\n",
        "\n",
        "      if self.mode == 'image':\n",
        "        for file in files:\n",
        "          image = imageio.imread(f'vis-data-simple/{file}/{file}_image.png')\n",
        "          img = np.moveaxis(img, 2, 0)\n",
        "          img = np.array(img)\n",
        "          data.append(img)\n",
        "          labels.append(self.gen_labels(file))\n",
        "      elif self.mode == 'sound':\n",
        "        for file in files:\n",
        "          with open(f'vis-data-simple/{file}/{file}_metadata.txt', 'r') as f:\n",
        "            n_sounds = int(f.readline())\n",
        "          for i in range(n_sounds):\n",
        "            img = imageio.imread(f'vis-data-simple/{file}/{file}_spec_{i}.png')\n",
        "            img = np.moveaxis(img, 2, 0)\n",
        "            img = img.resize((224, 224))\n",
        "            img = np.array(img)\n",
        "            data.append(img)\n",
        "            labels.append(self.gen_labels(file, i))\n",
        "      \n",
        "      data = torch.Tensor(data)\n",
        "      labels = torch.Tensor(labels)\n",
        "      \n",
        "      return data, labels\n",
        "    \n",
        "    def gen_labels(file, idx=None):\n",
        "      ret = np.zeroes((17, 1))\n",
        "      if idx is None:\n",
        "        with open(f'vis-data-simple/{file}/{file}_metadata.txt', 'r') as f:\n",
        "          n_sounds = int(f.readline())\n",
        "        for i in range(n_sounds):\n",
        "          with open(f'vis-data-simple/{file}/{file}_labels_{i}.txt', 'r') as f:\n",
        "            vals = f.readlines()\n",
        "            mat_id = int(vals[0])\n",
        "            ret[mat_id] = 1\n",
        "      else:\n",
        "        with open(f'vis-data-simple/{file}/{file}_labels_{idx}.txt', 'r') as f:\n",
        "          vals = f.readlines()\n",
        "          mat_id = int(vals[0])\n",
        "          ret[mat_id] = 1\n",
        "      return ret\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "      return self.data['inputs'][item], self.data['targets'][item]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.n"
      ],
      "metadata": {
        "id": "4awkTjogAozL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "# You might not have tqdm, which gives you nice progress bars\n",
        "!pip install tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import copy\n",
        "import pandas as pd\n",
        "import PIL \n",
        "from torch.utils.data import DataLoader\n",
        "  \n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using the GPU!\")\n",
        "else:\n",
        "    print(\"WARNING: Could not find GPU! Using CPU only\")\n",
        "    print(\"You may want to try to use the GPU in Google Colab by clicking in:\")\n",
        "    print(\"Runtime > Change Runtime type > Hardware accelerator > GPU.\")"
      ],
      "metadata": {
        "id": "mlRsSK9gUvN4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44c9b29-d0c7-4e59-8d68-80aed23649e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Using the GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(model_name, num_classes, resume_from = None, use_pretrained = False):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    # The model (nn.Module) to return\n",
        "    model_ft = None\n",
        "    # The input image is expected to be (input_size, input_size)\n",
        "    input_size = 0\n",
        "    \n",
        "    # By default, all parameters will be trained (useful when you're starting from scratch)\n",
        "    # Within this function you can set .requires_grad = False for various parameters, if you\n",
        "    # don't want to learn them\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "        \n",
        "    elif model_name == \"resnet50\":\n",
        "        \"\"\" Resnet50\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet50(pretrained=use_pretrained)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
        "        input_size = 224\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"Invalid model name!\")\n",
        "    \n",
        "    if resume_from is not None:\n",
        "        print(\"Loading weights from %s\" % resume_from)\n",
        "        model_ft.load_state_dict(torch.load(resume_from))\n",
        "    \n",
        "    return model_ft, input_size"
      ],
      "metadata": {
        "id": "72upn3XWU0wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.functional import to_grayscale\n",
        "\n",
        "def get_image_transforms():\n",
        "    # How to transform the image when you are loading them.\n",
        "    # you'll likely want to mess with the transforms on the training set.\n",
        "    \n",
        "    # we convert the image to a [C,H,W] tensor, then normalize it to values with a given mean/stdev. These normalization constants\n",
        "    # are derived from aggregating lots of data and happen to produce better results.\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    return transform\n",
        "\n",
        "def get_dataloaders(input_size, batch_size, mode, shuffle = True, transform=get_image_transforms()):\n",
        "    data_transforms = {\n",
        "        'train': transform,\n",
        "        'val': transform,\n",
        "        'test': transform\n",
        "    }\n",
        "\n",
        "    train_data = VisSoundsDataset(split = 'train', mode = mode)\n",
        "    train_data = VisSoundsDataset(split = 'val', mode = mode)\n",
        "    test_data = VisSoundsDataset(split = 'test', mode = mode)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = shuffle, num_workers = 4)\n",
        "    val_loader = DataLoader(train_data, batch_size = batch_size, shuffle = False, num_workers = 4)\n",
        "    test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = False, num_workers = 4)\n",
        "    loaders = {\n",
        "        'train': train_loader,\n",
        "        'val': val_loader,\n",
        "        'test': test_loader\n",
        "    }\n",
        "    return loaders"
      ],
      "metadata": {
        "id": "6UVO7iipfUS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, save_dir = None, save_all_epochs=False, num_epochs=25):\n",
        "    '''\n",
        "    model: The NN to train\n",
        "    dataloaders: A dictionary containing at least the keys \n",
        "                 'train','val' that maps to Pytorch data loaders for the dataset\n",
        "    criterion: The Loss function\n",
        "    optimizer: The algorithm to update weights \n",
        "               (Variations on gradient descent)\n",
        "    num_epochs: How many epochs to train for\n",
        "    save_dir: Where to save the best model weights that are found, \n",
        "              as they are found. Will save to save_dir/weights_best.pt\n",
        "              Using None will not write anything to disk\n",
        "    save_all_epochs: Whether to save weights for ALL epochs, not just the best\n",
        "                     validation error epoch. Will save to save_dir/weights_e{#}.pt\n",
        "    '''\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "    train_acc_history = []\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            # TQDM has nice progress bars\n",
        "            for inputs, labels in tqdm(dataloaders[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    outputs = model(inputs)\n",
        "                    outputs = outputs.to(device)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # torch.max outputs the maximum value, and its index\n",
        "                    # Since the input is batched, we take the max along axis 1\n",
        "                    # (the meaningful outputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backprop + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "            \n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'train':\n",
        "                train_acc_history.append(epoch_acc)\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "            if save_all_epochs:\n",
        "                torch.save(model.state_dict(), os.path.join(save_dir, f'weights_{epoch}.pt'))\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # save and load best model weights\n",
        "    torch.save(best_model_wts, os.path.join(save_dir, 'weights_best_val_acc.pt'))\n",
        "    torch.save(model.state_dict(), os.path.join(save_dir, 'weights_last.pt'.format(epoch)))\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history, train_acc_history"
      ],
      "metadata": {
        "id": "wlxJ7Xb6fcS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_optimizer(model, learning_rate, print_parameters=False):\n",
        "    # Get all the parameters\n",
        "    params_to_update = model.parameters()\n",
        "    if print_parameters:\n",
        "      print(\"Params to learn:\")\n",
        "      for name, param in model.named_parameters():\n",
        "          if param.requires_grad == True:\n",
        "              print(\"\\t\",name)\n",
        "\n",
        " \n",
        "    optimizer = optim.SGD(params_to_update, lr=learning_rate, momentum=0.9)\n",
        "    return optimizer\n",
        "\n",
        "def get_loss():\n",
        "    # Create an instance of the loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    return criterion"
      ],
      "metadata": {
        "id": "gkkw0npgfjHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet]\n",
        "# You can add your own, or modify these however you wish!\n",
        "model_name = 'resnet'\n",
        "\n",
        "# Number of classes in the dataset, normal, benign, malignant\n",
        "num_classes = 17\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 32\n",
        "\n",
        "# Shuffle the input data?\n",
        "shuffle_datasets = True\n",
        "\n",
        "# Number of epochs to train for \n",
        "num_epochs = 40\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 1e-3\n",
        "\n",
        "### IO\n",
        "# Path to a model file to use to start weights at\n",
        "resume_from = None\n",
        "\n",
        "# Whether to use a pretrained model, trained for classification in Imagenet-1k \n",
        "pretrained = False\n",
        "\n",
        "# Save all epochs so that you can select the model from a particular epoch\n",
        "save_all_epochs = False\n",
        "\n",
        "# Whether to use early stopping (load the model with best accuracy), or not\n",
        "early_stopping = True\n",
        "\n",
        "# Directory to save weights to\n",
        "save_dir = '/trained_model_1'\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "-Ay1Uh_wfmaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sound Only\n",
        "model_1, input_size = initialize_model(model_name = model_name, num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)"
      ],
      "metadata": {
        "id": "tLD6gjfJf7Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = get_dataloaders(input_size, batch_size, 'sound', shuffle_datasets)\n",
        "criterion = get_loss()\n",
        "\n",
        "# Move the model to the gpu if needed\n",
        "model_1 = model_1.to(device)\n",
        "\n",
        "optimizer_1 = make_optimizer(model_1, learning_rate)\n",
        "\n",
        "hello"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "zAZTGV3Z8gXj",
        "outputId": "6efae112-c838-45ec-b697-3a76488ef9d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-295bc03c69ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sound'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle_datasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Move the model to the gpu if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-e0d2c7ca5fe4>\u001b[0m in \u001b[0;36mget_dataloaders\u001b[0;34m(input_size, batch_size, mode, shuffle, transform)\u001b[0m\n\u001b[1;32m     21\u001b[0m     }\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisSoundsDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisSoundsDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisSoundsDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-29bcd0231a7b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, split, mode)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'VisSoundsDataset' object has no attribute 'load_data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model!\n",
        "trained_model_1, validation_history_1, train_history_1 = train_model(model=model_1, \n",
        "                                                                     dataloaders=dataloaders, \n",
        "                                                                     criterion=criterion, \n",
        "                                                                     optimizer=optimizer_1,\n",
        "                                                                     save_dir=save_dir, \n",
        "                                                                     save_all_epochs=save_all_epochs, \n",
        "                                                                     num_epochs=num_epochs)\n",
        "del model_1, optimizer_1, trained_model_1"
      ],
      "metadata": {
        "id": "KwgN_FLuy5tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your final model, that we will use for the rest of the PSET.\n",
        "if early_stopping:\n",
        "  weights_file = save_dir + '/weights_best_val_acc.pt'\n",
        "else:\n",
        "  weights_file = save_dir + '/weights_last.pt'\n",
        "model_yours, _ = initialize_model(model_name = model_name, num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)\n",
        "\n",
        "# Move the model to the gpu if needed\n",
        "model_yours = model_yours.to(device)\n",
        "\n",
        "# Load weights for model_yours\n",
        "model_yours.load_state_dict(torch.load(weights_file))\n",
        "\n",
        "# set models to eval mode\n",
        "model_yours = model_yours.eval()\n",
        "\n"
      ],
      "metadata": {
        "id": "xPWWDAe9zFsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, criterion, is_labelled = False, generate_labels = True, k = 5):\n",
        "    # If is_labelled, we want to compute loss, top-1 accuracy and top-5 accuracy\n",
        "    # If generate_labels, we want to output the actual labels\n",
        "    # Set the model to evaluate mode\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    running_top1_correct = 0\n",
        "    running_top5_correct = 0\n",
        "    predicted_labels = []\n",
        "    gt_labels = []\n",
        "\n",
        "    # Iterate over data.\n",
        "    # TQDM has nice progress bars\n",
        "    for inputs, labels in tqdm(dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        tiled_labels = torch.stack([labels.data for i in range(k)], dim=1) \n",
        "        # Makes this to calculate \"top 5 prediction is correct\"\n",
        "        # [[label1 label1 label1 label1 label1], [label2 label2 label2 label label2]]\n",
        "\n",
        "        # forward\n",
        "        # track history if only in train\n",
        "        with torch.set_grad_enabled(False):\n",
        "            # Get model outputs and calculate loss\n",
        "            outputs = model(inputs)\n",
        "            if is_labelled:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            # torch.topk outputs the maximum values, and their indices\n",
        "            # Since the input is batched, we take the max along axis 1\n",
        "            # (the meaningful outputs)\n",
        "            _, preds = torch.topk(outputs, k=k, dim=1)\n",
        "            if generate_labels:\n",
        "                # We want to store these results\n",
        "                nparr = preds.cpu().detach().numpy()\n",
        "                predicted_labels.extend([list(nparr[i]) for i in range(len(nparr))])\n",
        "                gt_labels.extend(np.array(labels.cpu()))\n",
        "\n",
        "        if is_labelled:\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            # Check only the first prediction\n",
        "            running_top1_correct += torch.sum(preds[:, 0] == labels.data)\n",
        "            # Check all 5 predictions\n",
        "            running_top5_correct += torch.sum(preds == tiled_labels)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    # Only compute loss & accuracy if we have the labels\n",
        "    if is_labelled:\n",
        "        epoch_loss = float(running_loss / len(dataloader.dataset))\n",
        "        epoch_top1_acc = float(running_top1_correct.double() / len(dataloader.dataset))\n",
        "        epoch_top5_acc = float(running_top5_correct.double() / len(dataloader.dataset))\n",
        "    else:\n",
        "        epoch_loss = None\n",
        "        epoch_top1_acc = None\n",
        "        epoch_top5_acc = None\n",
        "    \n",
        "    # Return everything\n",
        "    return epoch_loss, epoch_top1_acc, gt_labels, predicted_labels  "
      ],
      "metadata": {
        "id": "wWA4JzO_zU0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data on the validation set\n",
        "# Setting this to false will be a little bit faster\n",
        "generate_validation_labels = True\n",
        "val_loss_yours, val_top1_yours, _, val_labels_yours = evaluate(model_yours, dataloaders['val'], criterion, is_labelled = True, generate_labels = generate_validation_labels, k = 1)\n",
        "# Get predictions for the test set\n",
        "test_loss_yours, test_top1_yours, _, test_labels_yours = evaluate(model_yours, dataloaders['test'], criterion, is_labelled = True, generate_labels = generate_validation_labels, k = 1)\n",
        "\n",
        "print(\"Your Trained model: \")\n",
        "print(\"Val Top-1 Accuracy: {}\".format(val_top1_yours))\n",
        "print(\"Test Top-1 Accuracy: {}\".format(test_top1_yours))"
      ],
      "metadata": {
        "id": "9FmNZTK2zVWY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}