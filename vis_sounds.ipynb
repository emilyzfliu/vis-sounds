{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vis_sounds.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUAUAl9GUmTf"
      },
      "outputs": [],
      "source": [
        "# imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGVIKB09Urcu",
        "outputId": "f0f58b0d-cd63-4c90-de27-ee7ce46a0ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "!apt install ffmpeg"
      ],
      "metadata": {
        "id": "wjnSkZKmIGMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "\n",
        "# From VGGSound\n",
        "# model\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from models import resnet\n",
        "\n",
        "# preprocess_audio\n",
        "import glob\n",
        "import multiprocessing\n",
        "import subprocess\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "# test\n",
        "from torch.optim import *\n",
        "import torchvision\n",
        "from torchvision.transforms import *\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "import csv\n",
        "from model import AVENet\n",
        "# from datasets import GetAudioVideoDataset\n",
        "\n",
        "# utils\n",
        "import os\n",
        "from scipy import stats\n",
        "from sklearn import metrics\n",
        "\n",
        "# eval\n",
        "from utils import *\n",
        "\n",
        "# dataloader\n",
        "import cv2\n",
        "from torchvision import transforms, utils\n",
        "import time\n",
        "from PIL import Image\n",
        "import sys\n",
        "from scipy import signal\n",
        "import random\n",
        "import soundfile as sf\n",
        "\n",
        "# ambient\n",
        "#downstream eval\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from shutil import copyfile\n",
        "\n",
        "import itertools\n",
        "import utils.models"
      ],
      "metadata": {
        "id": "ckwvKxcW4eED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check requirements for following\n",
        "# dependencies = [\n",
        "# 'Cython==0.29.13',\n",
        "# 'decorator==4.1.2',\n",
        "# 'future==0.18.2',\n",
        "# 'h5py==2.7.1',\n",
        "# 'imageio==2.6.1',\n",
        "# 'librosa==0.7.1',\n",
        "# 'matplotlib==2.1.0',\n",
        "# 'numba==0.46.0',\n",
        "# 'numpy==1.17.3',\n",
        "# 'opencv-contrib-python==3.4.2.17',\n",
        "# 'opencv-python==3.4.2.17',\n",
        "# 'pandas==0.22.0',\n",
        "# 'pathlib==1.0.1',\n",
        "# 'Pillow==6.2.1',\n",
        "# 'scikit-learn==0.21.3',\n",
        "# 'scipy==1.3.2',\n",
        "# 'torch==1.3.1+cu92',\n",
        "# 'torchvision==0.4.2+cu92',\n",
        "# 'tqdm==4.19.4',\n",
        "# 'youtube-dl==2019.11.5      \n",
        "# ]\n",
        "\n",
        "pip freeze -r requirements.txt"
      ],
      "metadata": {
        "id": "B6yohlRkL490"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Overview:\n",
        "\n",
        "1. Sounds from Images, Images from Sounds\n",
        "  * We will start with the ImageNet neural net architecture for training our model to predict sounds from images as in (https://arxiv.org/pdf/1512.08512.pdf).\n",
        "  * We will likely use open-source pretrained weights and replace the classification layers with sound identification layers (TODO: Find ImageNet pretrained weights).\n",
        "  * We additionally hope to attempt an image-sound-image reconstruction by consecutively passing through the two models.\n",
        "2. Generated image/sound as input in downstream tasks: can we augment visual datasets by generating new images from sounds?\n",
        "  * network-generated sounds + raw images in sound-supervised statistical summary task (https://arxiv.org/pdf/1608.07017.pdf)"
      ],
      "metadata": {
        "id": "AtA0cG7BVLY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Utils"
      ],
      "metadata": {
        "id": "qBibN6Ju-Bx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "\n",
        "    def __init__(self, path, header):\n",
        "        self.log_file = open(path, 'w')\n",
        "        self.logger = csv.writer(self.log_file, delimiter='\\t')\n",
        "\n",
        "        self.logger.writerow(header)\n",
        "        self.header = header\n",
        "\n",
        "    def __del(self):\n",
        "        self.log_file.close()\n",
        "\n",
        "    def log(self, values):\n",
        "        write_values = []\n",
        "        for col in self.header:\n",
        "            assert col in values\n",
        "            write_values.append(values[col])\n",
        "\n",
        "        self.logger.writerow(write_values)\n",
        "        self.log_file.flush()\n",
        "\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1, 5)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res, pred\n",
        "\n",
        "\n",
        "def reverseTransform(img):\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    if len(img.shape) == 5:\n",
        "        for i in range(3):\n",
        "            img[:, i, :, :, :] = img[:, i, :, :, :]*std[i] + mean[i]\n",
        "    else:\n",
        "        for i in range(3):\n",
        "            img[:, i, :, :] = img[:, i, :, :]*std[i] + mean[i]\n",
        "    return img\n",
        "\n",
        "\n",
        "def d_prime(auc):\n",
        "    standard_normal = stats.norm()\n",
        "    d_prime = standard_normal.ppf(auc) * np.sqrt(2.0)\n",
        "    return d_prime\n",
        "\n",
        "\n",
        "def calculate_stats(output, target):\n",
        "    \"\"\"Calculate statistics including mAP, AUC, etc.\n",
        "    Args:\n",
        "      output: 2d array, (samples_num, classes_num)\n",
        "      target: 2d array, (samples_num, classes_num)\n",
        "    Returns:\n",
        "      stats: list of statistic of each class.\n",
        "    \"\"\"\n",
        "\n",
        "    classes_num = target.shape[-1]\n",
        "    stats = []\n",
        "\n",
        "    # Class-wise statistics\n",
        "    for k in range(classes_num):\n",
        "\n",
        "        # Average precision\n",
        "        avg_precision = metrics.average_precision_score(\n",
        "            target[:, k], output[:, k], average=None)\n",
        "\n",
        "        # AUC\n",
        "        auc = metrics.roc_auc_score(target[:, k], output[:, k], average=None)\n",
        "\n",
        "        # Precisions, recalls\n",
        "        (precisions, recalls, thresholds) = metrics.precision_recall_curve(\n",
        "            target[:, k], output[:, k])\n",
        "\n",
        "        # FPR, TPR\n",
        "        (fpr, tpr, thresholds) = metrics.roc_curve(target[:, k], output[:, k])\n",
        "\n",
        "        save_every_steps = 1000     # Sample statistics to reduce size\n",
        "        dict = {'precisions': precisions[0::save_every_steps],\n",
        "                'recalls': recalls[0::save_every_steps],\n",
        "                'AP': avg_precision,\n",
        "                'fpr': fpr[0::save_every_steps],\n",
        "                'fnr': 1. - tpr[0::save_every_steps],\n",
        "                'auc': auc}\n",
        "        stats.append(dict)\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "id": "JZNEW4VK-ARv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets\n",
        "We use the following datasets:\n",
        "1. Owens: https://andrewowens.com/vis/hits.html\n",
        "2. VoxCeleb 1, 2: https://www.robots.ox.ac.uk/~vgg/data/voxceleb/\n",
        "3. VGG Sound: https://www.robots.ox.ac.uk/~vgg/data/vggsound/"
      ],
      "metadata": {
        "id": "2lGTQDTOZh02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataloader"
      ],
      "metadata": {
        "id": "awF9ZFPN6xp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GetAudioVideoDataset(Dataset):\n",
        "\n",
        "    def __init__(self, args, mode='train', transforms=None):\n",
        "        data2path = {}\n",
        "        classes = []\n",
        "        classes_ = []\n",
        "        data = []\n",
        "        data2class = {}\n",
        "\n",
        "        with open(args.csv_path + 'stat.csv') as f:\n",
        "            csv_reader = csv.reader(f)\n",
        "            for row in csv_reader:\n",
        "                classes.append(row[0])\n",
        "\n",
        "        with open(args.csv_path  + args.test) as f:\n",
        "            csv_reader = csv.reader(f)\n",
        "            for item in csv_reader:\n",
        "                if item[1] in classes and os.path.exists(args.data_path + item[0][:-3] + 'wav')::\n",
        "                    data.append(item[0])\n",
        "                    data2class[item[0]] = item[1]\n",
        "\n",
        "        self.audio_path = args.data_path \n",
        "        self.mode = mode\n",
        "        self.transforms = transforms\n",
        "        self.classes = sorted(classes)\n",
        "        self.data2class = data2class\n",
        "\n",
        "        # initialize audio transform\n",
        "        self._init_atransform()\n",
        "        #  Retrieve list of audio and video files\n",
        "        self.video_files = []\n",
        "        \n",
        "        for item in data:\n",
        "            self.video_files.append(item)\n",
        "        print('# of audio files = %d ' % len(self.video_files))\n",
        "        print('# of classes = %d' % len(self.classes))\n",
        "\n",
        "\n",
        "    def _init_atransform(self):\n",
        "        self.aid_transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_files)  \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        wav_file = self.video_files[idx]\n",
        "        # Audio\n",
        "        samples, samplerate = sf.read(self.audio_path + wav_file[:-3]+'wav')\n",
        "\n",
        "        # repeat in case audio is too short\n",
        "        resamples = np.tile(samples,10)[:160000]\n",
        "\n",
        "        resamples[resamples > 1.] = 1.\n",
        "        resamples[resamples < -1.] = -1.\n",
        "        frequencies, times, spectrogram = signal.spectrogram(resamples, samplerate, nperseg=512,noverlap=353)\n",
        "        spectrogram = np.log(spectrogram+ 1e-7)\n",
        "\n",
        "        mean = np.mean(spectrogram)\n",
        "        std = np.std(spectrogram)\n",
        "        spectrogram = np.divide(spectrogram-mean,std+1e-9)\n",
        "\n",
        "        return spectrogram, resamples,self.classes.index(self.data2class[wav_file]),wav_file"
      ],
      "metadata": {
        "id": "mlRsSK9gUvN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocess Audio"
      ],
      "metadata": {
        "id": "QX2GXwCb9pKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_arguments():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        '--video_input',\n",
        "        default='',\n",
        "        type=str,\n",
        "        help='Input directory path of videos or audios')\n",
        "    parser.add_argument(\n",
        "        '--audio_output',\n",
        "        default='',\n",
        "        type=str,\n",
        "        help='Output directory path of videos')\n",
        "    return parser.parse_args() \n",
        "\n",
        "def convert(v):\n",
        "    subprocess.check_call([\n",
        "    'ffmpeg',\n",
        "    '-n',\n",
        "    '-i', v,\n",
        "    '-acodec', 'pcm_s16le',\n",
        "    '-ac','1',\n",
        "    '-ar','16000',\n",
        "    args.audio_output + '%s.wav' % v.split('/')[-1][:-4]])\n",
        "\n",
        "def obtain_list():\n",
        "    files = []\n",
        "    txt = glob.glob(args.video_input + '/*.mp4') # '/*.flac'\n",
        "    for item in txt:\n",
        "        files.append(item)\n",
        "    return files\n",
        "\n",
        "args = get_arguments()\n",
        "p = multiprocessing.Pool(32)\n",
        "p.map(convert, obtain_list())"
      ],
      "metadata": {
        "id": "72upn3XWU0wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual Sound"
      ],
      "metadata": {
        "id": "d8bnMmC54NAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Resnet"
      ],
      "metadata": {
        "id": "UzI4H8ip-ph8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes ,stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        \n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, pool = 'avgpool',zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.pool = pool\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        if self.pool == 'avgpool':\n",
        "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        \n",
        "            self.fc = nn.Linear(512 * block.expansion, num_classes) # 8192\n",
        "        elif self.pool == 'vlad':\n",
        "            self.avgpool = NetVLAD()\n",
        "            self.fc_ = nn.Linear(8192 * block.expansion, num_classes)\n",
        "        \n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.normal_(m.weight, mean=1, std=0.02)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                    conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                    norm_layer(planes * block.expansion),\n",
        "                )\n",
        "           \n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        if self.pool == 'avgpool':\n",
        "            x = self.fc(x)\n",
        "        elif self.pool == 'vlad':\n",
        "            x = self.fc_(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class NetVLAD(nn.Module):\n",
        "    \"\"\"NetVLAD layer implementation\"\"\"\n",
        "\n",
        "    def __init__(self, num_clusters=16, dim=512, alpha=100.0,\n",
        "                 normalize_input=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_clusters : int\n",
        "                The number of clusters\n",
        "            dim : int\n",
        "                Dimension of descriptors\n",
        "            alpha : float\n",
        "                Parameter of initialization. Larger value is harder assignment.\n",
        "            normalize_input : bool\n",
        "                If true, descriptor-wise L2 normalization is applied to input.\n",
        "        \"\"\"\n",
        "        super(NetVLAD, self).__init__()\n",
        "        self.num_clusters = num_clusters\n",
        "        self.dim = dim\n",
        "        self.alpha = alpha\n",
        "        self.normalize_input = normalize_input\n",
        "        self.conv = nn.Conv2d(dim, num_clusters, kernel_size=(1, 1), bias=True)\n",
        "        self.centroids = nn.Parameter(torch.rand(num_clusters, dim))\n",
        "        self._init_params()\n",
        "\n",
        "    def _init_params(self):\n",
        "        self.conv.weight = nn.Parameter(\n",
        "            (2.0 * self.alpha * self.centroids).unsqueeze(-1).unsqueeze(-1)\n",
        "        )\n",
        "        self.conv.bias = nn.Parameter(\n",
        "            - self.alpha * self.centroids.norm(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C = x.shape[:2]\n",
        "\n",
        "        if self.normalize_input:\n",
        "            x = F.normalize(x, p=2, dim=1)  # across descriptor dim\n",
        "\n",
        "        # soft-assignment\n",
        "        soft_assign = self.conv(x).view(N, self.num_clusters, -1)\n",
        "        soft_assign = F.softmax(soft_assign, dim=1)\n",
        "\n",
        "        x_flatten = x.view(N, C, -1)\n",
        "        \n",
        "        # calculate residuals to each clusters\n",
        "        residual = x_flatten.expand(self.num_clusters, -1, -1, -1).permute(1, 0, 2, 3) - \\\n",
        "            self.centroids.expand(x_flatten.size(-1), -1, -1).permute(1, 2, 0).unsqueeze(0)\n",
        "        residual *= soft_assign.unsqueeze(2)\n",
        "        vlad = residual.sum(dim=-1)\n",
        "\n",
        "        vlad = F.normalize(vlad, p=2, dim=2)  # intra-normalization\n",
        "        vlad = vlad.view(x.size(0), -1)  # flatten\n",
        "        vlad = F.normalize(vlad, p=2, dim=1)  # L2 normalize\n",
        "\n",
        "        return vlad\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "def resnet34(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "def resnet50(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "def resnet101(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "def resnet152(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "def resnext50_32x4d(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNeXt-50 32x4d model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['groups'] = 32\n",
        "    kwargs['width_per_group'] = 4\n",
        "    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\n",
        "                   pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def resnext101_32x8d(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNeXt-101 32x8d model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['groups'] = 32\n",
        "    kwargs['width_per_group'] = 8\n",
        "    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\n",
        "                   pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def wide_resnet50_2(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a Wide ResNet-50-2 model.\n",
        "    The model is the same as ResNet except for the bottleneck number of channels\n",
        "    which is twice larger in every block. The number of channels in outer 1x1\n",
        "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
        "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['width_per_group'] = 64 * 2\n",
        "    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3],\n",
        "                   pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def wide_resnet101_2(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a Wide ResNet-101-2 model.\n",
        "    The model is the same as ResNet except for the bottleneck number of channels\n",
        "    which is twice larger in every block. The number of channels in outer 1x1\n",
        "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
        "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['width_per_group'] = 64 * 2\n",
        "    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3],\n",
        "                   pretrained, progress, **kwargs)"
      ],
      "metadata": {
        "id": "a_S93tKb7mez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model"
      ],
      "metadata": {
        "id": "T2BBHkFg-u1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AVENet(nn.Module):\n",
        "\n",
        "    def __init__(self,args):\n",
        "        super(AVENet, self).__init__()\n",
        "        self.audnet = Resnet(args)\n",
        "\n",
        "    def forward(self, audio):\n",
        "        aud = self.audnet(audio)\n",
        "        return aud\n",
        "\n",
        "\n",
        "def Resnet(opt):\n",
        "\n",
        "    assert opt.model_depth in [10, 18, 34, 50, 101, 152, 200]\n",
        "\n",
        "    if opt.model_depth == 10:\n",
        "        model = resnet.resnet10(\n",
        "            num_classes=opt.n_classes)\n",
        "    elif opt.model_depth == 18:\n",
        "        model = resnet.resnet18(\n",
        "            num_classes=opt.n_classes,\n",
        "            pool=opt.pool)\n",
        "    elif opt.model_depth == 34:\n",
        "        model = resnet.resnet34(\n",
        "            num_classes=opt.n_classes,\n",
        "            pool=opt.pool)\n",
        "    elif opt.model_depth == 50:\n",
        "        model = resnet.resnet50(\n",
        "            num_classes=opt.n_classes,\n",
        "            pool=opt.pool)\n",
        "    elif opt.model_depth == 101:\n",
        "        model = resnet.resnet101(\n",
        "            num_classes=opt.n_classes)\n",
        "    elif opt.model_depth == 152:\n",
        "        model = resnet.resnet152(\n",
        "            num_classes=opt.n_classes)\n",
        "    elif opt.model_depth == 200:\n",
        "        model = resnet.resnet200(\n",
        "            num_classes=opt.n_classes)\n",
        "    return model "
      ],
      "metadata": {
        "id": "XM9U6wVK4OL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test"
      ],
      "metadata": {
        "id": "QNZNu48o9wkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_arguments():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        '--data_path',\n",
        "        #default='/scratch/shared/beegfs/hchen/train_data/VGGSound_final/audio/',\n",
        "        default='https://raw.githubusercontent.com/hche11/VGGSound/master/data/vggsound.csv',\n",
        "        type=str,\n",
        "        help='Directory path of data')\n",
        "    parser.add_argument(\n",
        "        '--result_path',\n",
        "        # default='/scratch/shared/beegfs/hchen/prediction/audioclassification/vggsound/resnet18/',\n",
        "        default=resnet18,\n",
        "        type=str,\n",
        "        help='Directory path of results')\n",
        "    parser.add_argument(\n",
        "        '--summaries',\n",
        "        # default='/scratch/shared/beegfs/hchen/epoch/audioclassification_f/resnet18_vlad/model.pth.tar',\n",
        "        default='',\n",
        "        type=str,\n",
        "        help='Directory path of pretrained model')\n",
        "    parser.add_argument(\n",
        "        '--pool',\n",
        "        default=\"vlad\",\n",
        "        type=str,\n",
        "        help= 'either vlad or avgpool')\n",
        "    parser.add_argument(\n",
        "        '--csv_path',\n",
        "        #default='./data/',\n",
        "        default='https://raw.githubusercontent.com/hche11/VGGSound/master/data/vggsound.csv',\n",
        "        type=str,\n",
        "        help='metadata directory')\n",
        "    \n",
        "    ## not sure if this argument is needed because test.csv is just a function within this notebook\n",
        "    # parser.add_argument(\n",
        "    #     '--test',\n",
        "    #     default='test.csv',\n",
        "    #     type=str,\n",
        "    #     help='test csv files')\n",
        "    parser.add_argument(\n",
        "        '--batch_size', \n",
        "        default=32, \n",
        "        type=int, \n",
        "        help='Batch Size')\n",
        "    parser.add_argument(\n",
        "        '--n_classes',\n",
        "        default=309,\n",
        "        type=int,\n",
        "        help=\n",
        "        'Number of classes')\n",
        "    parser.add_argument(\n",
        "        '--model_depth',\n",
        "        default=18,\n",
        "        type=int,\n",
        "        help='Depth of resnet (10 | 18 | 34 | 50 | 101)')\n",
        "    parser.add_argument(\n",
        "        '--resnet_shortcut',\n",
        "        default='B',\n",
        "        type=str,\n",
        "        help='Shortcut type of resnet (A | B)')\n",
        "    return parser.parse_args() \n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = get_arguments()\n",
        "\n",
        "    # create prediction directory if not exists\n",
        "    if not os.path.exists(args.result_path):\n",
        "        os.mkdir(args.result_path)\n",
        "\n",
        "    # init network\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "    model= AVENet(args) \n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.cuda()\n",
        "    \n",
        "    # load pretrained models\n",
        "    checkpoint = torch.load(args.summaries)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    print('load pretrained model.')\n",
        "\n",
        "    # create dataloader\n",
        "    testdataset = GetAudioVideoDataset(args,  mode='test')\n",
        "    testdataloader = DataLoader(testdataset, batch_size=args.batch_size, shuffle=False,num_workers = 16)\n",
        "\n",
        "    softmax = nn.Softmax(dim=1)\n",
        "    print(\"Loaded dataloader.\")\n",
        "\n",
        "    model.eval()\n",
        "    for step, (spec, audio, label, name) in enumerate(testdataloader):\n",
        "        print('%d / %d' % (step,len(testdataloader) - 1))\n",
        "        spec = Variable(spec).cuda()\n",
        "        label = Variable(label).cuda()\n",
        "        aud_o = model(spec.unsqueeze(1).float())\n",
        "\n",
        "        prediction = softmax(aud_o)\n",
        "\n",
        "        for i, item in enumerate(name):\n",
        "            np.save(args.result_path + '/%s.npy' % item,prediction[i].cpu().data.numpy())\n",
        "\n",
        "            # print example scores \n",
        "            # print('%s, label : %s, prediction score : %.3f' % (\n",
        "            #     name[i][:-4], testdataset.classes[label[i]], prediction[i].cpu().data.numpy()[label[i]]))\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "mvyeWQa09vlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ambient"
      ],
      "metadata": {
        "id": "s0QjmbOMGaPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ],
      "metadata": {
        "id": "X_2Bm-f_LMcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gXDZdE5FGW1Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}